
# ------------------------------------------------------------------------------------------------------
# A FAIRE EN PRODUCTION
# supprimer tqdm en prod - refaire un environnement virtuel avec requirement txt propre
# attention pour administrateur ceux qui vienne de csv ont pas de role mais on peut deduire de extra_keywords
# ------------------------------------------------------------------------------------------------------
# failed urls
# faire les loggers
# Pour ton cas (Moniteur belge, plusieurs juridictions, mÃªmes champs globaux) :
# â¤ Garde un seul index global.
# â¤ Ajoute une pondÃ©ration contextuelle dans la recherche ou un champ fiabilitÃ©_nom_trib_entreprise.
# On va dÃ©tailler calmement, car la diffÃ©rence entre raise et sys.exit() est fondamentale
# en architecture logicielle, surtout en prod.
# Commandes :
#            (venv) C:\Users\32471\ScrapperCJCE>del cache_bce\bce_indexes.pkl
#            (venv) C:\Users\32471\ScrapperCJCE>del .cache\denomination.denoms
# ------------------------------------------------------------------------------------------------------
# --- Imports standards ---
import concurrent.futures
import json
import locale
import logging
import os
import time
import re
import sys
import csv
from collections import defaultdict
from datetime import date
from functools import wraps

# --- BibliothÃ¨ques tierces ---
import meilisearch
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from tqdm import tqdm

# --- Modules internes au projet ---
from logger_config import setup_logger, setup_dynamic_logger
from Constante.mesconstantes import BASE_URL
# EXTRACTION DES MOTS CLEFS
from Extraction.Keyword.tribunal_entreprise_keyword import detect_tribunal_entreprise_keywords
from Extraction.Keyword.tribunal_premiere_instance_keyword import detect_tribunal_premiere_instance_keywords
from Extraction.Keyword.cour_appel_keyword import detect_courappel_keywords
from Extraction.Keyword.radiation_keyword import detect_radiations_keywords
# EXTRACTION DES MANDATAIRES (curateurs/liquidateurs)
from Extraction.Gerant.extraction_administrateurs import extract_administrateur
from Extraction.MandataireJustice.extraction_mandataire_justice_gen import trouver_personne_dans_texte
# EXTRACTION DATE JUGEMENT
from Extraction.Dates.extraction_date_jugement import extract_jugement_date
from Utilitaire.ConvertDateToMeili import convertir_date
from Utilitaire.outils.MesOutils import detect_erratum, extract_numero_tva, \
    extract_clean_text, clean_url, generate_doc_hash_from_html, \
    clean_date_jugement, extract_nrn_variants, norm_er, \
    format_bce, chemin_csv, dedupe_admins, \
    normaliser_espaces_invisibles, charger_indexes_bce, \
    verifier_tva_belge, verifier_date_doc
from ParserMB.MonParser import find_linklist_in_items, retry
# Base de donnÃ©es POSTGRE
from BaseDeDonnees.creation_tables import create_table_moniteur
from BaseDeDonnees.insertion_moniteur import insert_documents_moniteur


def main():
    # ------------------------------------------------------------------------------------------------------------------
    # CONFIGURATION DE LA PÃ‰RIODE ET DES VARIABLES DE SCRAPING ( Ã  ameliorer)
    # ------------------------------------------------------------------------------------------------------------------
    assert len(sys.argv) == 2, "Usage: python MainScrapper.py \"mot+clef\""
    keyword = sys.argv[1]
    from_date = date.fromisoformat("2025-10-15")  # dÃ©but
    to_date = date.fromisoformat("2025-01-20")  # date.today()  # fin
    locale.setlocale(locale.LC_TIME, "fr_FR.UTF-8")
    # --------------------------------------------------------------------------------------------------------------
    #                 CHARGEMENT DES INDEX BCE
    # --------------------------------------------------------------------------------------------------------------
    print("[ğŸ“¦] Chargement initial des indexes BCEâ€¦")
    denom_index, personnes_physiques, address_index, enterprise_index, establishment_index = charger_indexes_bce()
    print("[âœ…] Index BCE chargÃ©s :", len(denom_index or {}), "entrÃ©es")
    # A vÃ©rifier / modifier
    if not denom_index:
        raise RuntimeError("âŒ Index BCE vides â€” vÃ©rifie le fichier CSV ou pickle.")
    # --------------------------------------------------------------------------------------------------------------
    #                 CONSTRUCTION FICHIER EXPORTS CONTENANT DONNEES NECESSAIRE A APPEL ANNEXES MONITEUR
    # --------------------------------------------------------------------------------------------------------------
    export_dir = "exports"
    os.makedirs(export_dir, exist_ok=True)
    csv_path = os.path.join(export_dir, "moniteur_enrichissement.csv")
    # --------------------------------------------------------------------------------------------------------------
    #                                    CONFIGURATION DES LOGGERS
    # --------------------------------------------------------------------------------------------------------------
    # **** LOGGER GENERAL
    logger = setup_logger("extraction", level=logging.DEBUG)
    logger.debug("âœ… Logger initialisÃ© dans le script principal.")
    # !!!! LOGGER ERREURS CRITIQUES (le parser devra eventuellement Ãªtre revu, changement structures pages moniteur)
    logger_critical = setup_dynamic_logger(name="critical", keyword=keyword, level=logging.DEBUG)
    logger_critical.debug("ğŸ” Logger 'critical' initialisÃ© "
                          "pour les champs"" obligatoires.")

    # ---- LOGGERS champs manquants
    # -- Fichier bce n'a pas les champs correspondant au num de tva extrait
    logger_champs_manquants_csv_bce = setup_dynamic_logger(name="champs_manquants_obligatoires",
                                                                keyword=keyword, level=logging.DEBUG)
    logger_champs_manquants_csv_bce.debug("ğŸ” Logger 'champs_manquants_obligatoires' initialisÃ© "
                                               "pour les champs"
                                               " obligatoires.")
    # -- Les champs de vÃ©ritÃ© Ã  intÃ©grer automatiquement dans Postgre ne sont pas prÃ©sents
    logger_champs_manquants_obligatoires = setup_dynamic_logger(name="champs_manquants_obligatoires1",
                                                                 keyword=keyword, level=logging.DEBUG)

    logger_champs_manquants_obligatoires.debug("ğŸ” Logger 'champs_manquants_obligatoires1' initialisÃ© "
                                                "pour les champs"
                                                " obligatoires.")

    # *** LOGGER Tva invalide Va falloir modifier ceci je pense
    logger_tva_invalide = setup_dynamic_logger(name="tva_invalide", keyword=keyword, level=logging.DEBUG)
    logger_tva_invalide.debug("ğŸ” Logger 'tva_invalide' initialisÃ© ")
    print(">>> CODE Ã€ JOUR")
    # ---------------------------------------------------------------------------------------------------------------------
    #                                          VARIABLES D ENVIRONNEMENT
    # ----------------------------------------------------------------------------------------------------------------------
    load_dotenv()
    meili_url = os.getenv("MEILI_URL")
    meili_key = os.getenv("MEILI_MASTER_KEY")
    index_name = os.getenv("INDEX_NAME")
    # ---------------------------------------------------------------------------------------------------------------------
    #                                          VERIFICATION DES ERREURS HTTP
    # -------------------------------------------------------------------------------------------------

    failed_urls = []
    # ---------------------------------------------------------------------------------------------------------------------
    #                                         INITIALISATION MEILISEARCH A MODIFIER EN PROD
    # -------------------------------------------------------------------------------------------------
    max_workers = 12
    TIMEOUT_RESULT = 90
    TIMEOUT_FUTURE = 120
    print("[INFO] Initialisation Meilisearch (au dÃ©but du script)â€¦")
    client = meilisearch.Client(meili_url, meili_key)
    try:
        index = client.get_index(index_name)
        delete_task = index.delete()
        client.wait_for_task(delete_task.task_uid)
        print(f"[ğŸ—‘ï¸] Ancien index '{index_name}' supprimÃ©.")
    except meilisearch.errors.MeilisearchApiError:
        print(f"[â„¹ï¸] Aucun index existant Ã  supprimer ({index_name}).")

    create_task = client.create_index(index_name, {"primaryKey": "id"})
    client.wait_for_task(create_task.task_uid)
    index = client.get_index(index_name)
    print(f"[âœ…] Index '{index_name}' prÃªt.")
    # remplacer en prod par ceci a retravailler eventuellement
    """client = meilisearch.Client(meili_url, meili_key)
try:
    index = client.get_index(index_name)
    delete_task = index.delete()
    client.wait_for_task(delete_task.task_uid)
    print(f"[ğŸ—‘ï¸] Ancien index '{index_name}' supprimÃ©.")
except meilisearch.errors.MeilisearchApiError:
    print(f"[â„¹ï¸] Aucun index existant Ã  supprimer ({index_name}).")

create_task = client.create_index(index_name, {"primaryKey": "id"})
client.wait_for_task(create_task.task_uid)
index = client.get_index(index_name)
print(f"[âœ…] Index '{index_name}' prÃªt.")
"""
    # --------------------------------------------------------------------------------------------------------------
    #                               FONCTIONS PRINCIPALES D EXTRACTION
    # --------------------------------------------------------------------------------------------------------------

    def get_page_amount(session, start_date, end_date, keyword):
        encoded = keyword.replace(" ", "+")
        today = date.today()
        url = (
            f"{BASE_URL}list.pl?"
            f"language=fr&"
            f"sum_date={today}&"
            f"page=&"
            f"pdd={start_date}&"
            f"pdf={end_date}&"
            f"choix1=et&"
            f"choix2=et&"
            f"exp={encoded}&"
            f"fr=f&"
            f"trier=promulgation"
        )
        response = retry(url, session)
        soup = BeautifulSoup(response.text, 'html.parser')
        last_link = soup.select_one("div.pagination-container a:last-child")
        if not last_link:
            return 1
        match = re.search(r'page=(\d+)', last_link["href"])
        return int(match.group(1)) if match else 1

    def ask_belgian_monitor(http_session, start_date, end_date, motclef):
        page_amount = get_page_amount(http_session, start_date, end_date, motclef)
        print(f"[INFO] Pages Ã  scraper pour '{motclef}': {page_amount}")
        link_list = []

        def process_page(page):
            encoded = motclef.replace(" ", "+")
            today = date.today()
            url = f'{BASE_URL}list.pl?language=fr&sum_date={today}&page={page}&pdd={start_date}&pdf={end_date}&choix1=et&choix2=et&exp={encoded}&fr=f&trier=promulgation'
            # NEW: crÃ©e une session locale dans ce thread (ne PAS rÃ©utiliser celle passÃ©e en argument)
            with requests.Session() as s:  # NEW
                response = retry(url, s)  # CHANGED (session -> s)
            soup = BeautifulSoup(response.text, 'html.parser')
            class_list = soup.find("div", class_="list")
            if not class_list:
                return
            for item in class_list.find_all(class_="list-item"):
                subtitle = item.find("p", class_="list-item--subtitle")
                subtitle_text = subtitle.get_text(strip=True) if subtitle else ""
                title_elem = item.find("a", class_="list-item--title")
                title = title_elem.get_text(strip=True) if title_elem else ""
                if motclef == "Liste+des+entites+enregistrees" and \
                        subtitle_text == "Service public fÃ©dÃ©ral Economie, P.M.E., Classes moyennes et Ã‰nergie":
                    find_linklist_in_items(item, motclef, link_list)
                elif motclef == "Conseil+d+'+Etat" and subtitle_text == "Conseil d'Ã‰tat" \
                        and title.lower().startswith("avis prescrit"):
                    find_linklist_in_items(item, motclef, link_list)
                elif motclef == "Cour+constitutionnelle" and subtitle_text == "Cour constitutionnelle":
                    find_linklist_in_items(item, motclef, link_list)

                elif motclef in "tribunal+de+premiere+instance":
                    if title.lower().startswith("tribunal de premiÃ¨re instance"):
                        find_linklist_in_items(item, motclef, link_list)

                elif motclef in "tribunal+de+l":
                    if (
                            title.lower().startswith("tribunal de l")

                    ):
                        find_linklist_in_items(item, motclef, link_list)

                elif motclef in "cour+d":
                    if (
                            title.lower().startswith("cour d'appel")

                    ):
                        find_linklist_in_items(item, motclef, link_list)

        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            list(tqdm(executor.map(process_page, range(1, page_amount + 1)), total=page_amount, desc="Pages"))

        return link_list

    # ------------------------------------------------------------------------------------------------------------------
    #             FONCTION PRINCIPALE EXTRACTION SCRAPER MONITEUR BELGE (publications tribunaux, listes radiation ...)
    # ------------------------------------------------------------------------------------------------------------------
    def scrap_informations_from_url(url, numac, date_doc, langue, keyword, title, subtitle):
        # ğŸš¨ Validation critique AVANT TOUT
        date_doc = verifier_date_doc(str(date_doc), url, logger_critical)
        if not date_doc:
            logger_critical.critical(f"[DROP] Document ignorÃ© : date_doc invalide | URL={url}")
            return None
        with requests.Session() as s:

            response = retry(url, session)
            if not response:
                print(f"[âŒ Abandon dÃ©finitif pour {url}]")
                return None  # âš ï¸ important : on sort si la page ne rÃ©pond jamais
            soup = BeautifulSoup(response.text, 'html.parser')
            extra_keywords = []

            main = soup.find("main", class_="page__inner page__inner--content article-text")
            if not main:
                return (
                    numac, date_doc, langue, "", url, keyword,
                    None, title, subtitle, None, None, None, None,
                    None, None, None, None, None
                )

            texte_brut = extract_clean_text(main, remove_links=False)
            date_jugement = None
            administrateur = None
            tvas = extract_numero_tva(texte_brut)
            tvas_valides = [t for t in tvas if format_bce(t)]
            denoms_by_bce = None
            adresses_by_bce = None
            denoms_by_ejustice = None
            adresses_by_ejustice = None

            doc_id = generate_doc_hash_from_html(texte_brut, date_doc)
            if detect_erratum(texte_brut):
                extra_keywords.append("erratum")

            # Cas normal
            # on va devoir deplacer nom
            if not date_jugement:
                date_jugement = extract_jugement_date(str(texte_brut))

            if re.search(r"tribunal[\s+_]+de[\s+_]+premiere[\s+_]+instance", keyword, flags=re.IGNORECASE | re.DOTALL):
                if not tvas_valides:
                    return None
                # A vÃ©rifier avant d'Ã©tendre aux autres
                if re.search(
                            r"\bsuccessions?\s*(?:de|vacantes?|en\s+d[Ã©e]sh[Ã©e]rences?)\b",
                            texte_brut,
                            flags=re.IGNORECASE):
                    return None
                # recherche des administrateurs avec les deux fonctions complementaires d'extraction d'administrateurs
                admins_csv = trouver_personne_dans_texte(
                      texte_brut,
                      chemin_csv("curateurs.csv"),
                      ["avocate", "avocat", "MaÃ®tre", "bureaux", "cabinet", "curateur", "liquidateur"]
                  )
                admins_rx = extract_administrateur(texte_brut)
                administrateur = dedupe_admins(admins_csv, admins_rx)
                detect_tribunal_premiere_instance_keywords(texte_brut, extra_keywords)
                if all("delai de contact" not in element for element in extra_keywords):
                      detect_tribunal_entreprise_keywords(texte_brut, extra_keywords)

            # -----------------------------
            # TRIB ENTREPRISE
            # -----------------------------
            if re.search(r"tribunal\s+de\s+l", keyword.replace("+", " "), flags=re.IGNORECASE):
                if not tvas_valides:
                    return None
                # recherche des administrateurs avec les deux fonctions complementaires d'extraction d'administrateurs
                admins_csv = trouver_personne_dans_texte(
                    texte_brut,
                    chemin_csv("curateurs.csv"),
                    ["avocate", "avocat", "MaÃ®tre", "bureaux", "cabinet", "curateur", "liquidateur"]
                )
                print("ğŸ§¾ ADMIN CSV =", admins_csv)

                admins_rx = extract_administrateur(texte_brut)
                print("ğŸ§© admins_rx =", admins_rx)

                administrateur = dedupe_admins(admins_csv, admins_rx)
                print("ğŸ¯ administrateur aprÃ¨s merge =", administrateur)
                detect_tribunal_entreprise_keywords(texte_brut, extra_keywords)

            # ----------------------------------------------------------------
            # ENTREPRISES RADIEES
            #     > Les entreprises radiees ne sont que des Personnes Morales
            # ----------------------------------------------------------------
            if re.search(r"Liste\s+des\s+entites\s+enregistrees", keyword.replace("+", " "), flags=re.IGNORECASE):
                if not tvas_valides:
                    return None
                detect_radiations_keywords(texte_brut, extra_keywords)

            # ------------ -----------------
            # COUR D'APPEL
            # -----------------------------
            # probleme? exemple 730689335c260eba1ea29679ae3c3277759008a345f24b544393667a7e7a8aba
            # pas vraiment je pense en fait (nom et nom trib sont avec du bruit et pas grave
            if re.search(r"cour\s+d", keyword.replace("+", " "), flags=re.IGNORECASE):
                if not tvas_valides:
                    return None
                # recherche des administrateurs avec les deux fonctions complementaires d'extraction d'administrateurs
                admins_csv = trouver_personne_dans_texte(
                    texte_brut,
                    chemin_csv("curateurs.csv"),
                    ["avocate", "avocat", "MaÃ®tre", "bureaux", "cabinet", "curateur"]
                )
                admins_rx = extract_administrateur(texte_brut)

                administrateur = dedupe_admins(admins_csv, admins_rx)
                detect_tribunal_entreprise_keywords(texte_brut, extra_keywords)
                detect_courappel_keywords(texte_brut, extra_keywords)
                detect_tribunal_premiere_instance_keywords(texte_brut, extra_keywords)

                # ğŸ§¼ Nettoyage spÃ©cifique Cour d'appel :
                # Dans les arrÃªts de cour d'appel, les noms de personnes physiques et les noms de sociÃ©tÃ©s
                # sont souvent extraits ensemble dans le champ `nom`.
                # Or, les sociÃ©tÃ©s sont dÃ©jÃ  prÃ©sentes dans `nom_trib_entreprise` (extrait via extract_noms_entreprises)
                # Ce bloc sert Ã  nettoyer `nom` pour ne garder QUE les personnes physiques :
                # on supprime des sous-champs (`records`, `canonicals`, `aliases_flat`)
                # tous les noms qui correspondent Ã  des entreprises dÃ©jÃ  dÃ©tectÃ©es.
                # â¤ Objectif : Ã©viter les doublons et le bruit dans Meilisearch ou PostgreSQL.

            return (
                numac, date_doc, langue, texte_brut, url, keyword,
                tvas, title, subtitle, extra_keywords, date_jugement,
                administrateur, doc_id, denoms_by_bce,
                adresses_by_bce, adresses_by_ejustice, denoms_by_ejustice
            )

    final = []
    with requests.Session() as session:

        raw_link_list = ask_belgian_monitor(session, from_date, to_date, keyword)
        link_list = raw_link_list  # on garde le nom pour compatibilitÃ©

        scrapped_data = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    scrap_informations_from_url, url, numac, date_doc, langue, keyword, title, subtitle
                ): url
                for (url, numac, date_doc, langue, keyword, title, subtitle) in link_list
            }

            total = len(futures)
            print(f"[DEBUG] Nombre total de futures : {total}")

            for future in tqdm(
                    concurrent.futures.as_completed(futures),
                    total=total,
                    desc=f"Scraping {keyword}",
            ):
                url = futures[future]
                try:
                    # â±ï¸ Timeout dur : 90 s max par tÃ¢che
                    result = future.result(timeout=90)
                    if result and isinstance(result, tuple) and len(result) >= 5:
                        scrapped_data.append(result)
                except concurrent.futures.TimeoutError:
                    print(f"[â°] Timeout sur {url}")
                    failed_urls.append(url)
                except Exception as e:
                    print(f"[âŒ] Erreur sur {url}: {type(e).__name__} â€“ {e}")
                    failed_urls.append(url)

        print(f"[DEBUG] Futures terminÃ©es : {sum(f.done() for f in futures)} / {len(futures)}")
        print(f"[ğŸ“‰] Pages Ã©chouÃ©es : {len(failed_urls)} / {len(link_list)}")

        # --- ğŸ” Sauvegarde pour relancer plus tard ---
        if failed_urls:
            with open("failed_urls.txt", "w", encoding="utf-8") as f:
                for u in failed_urls:
                    f.write(u + "\n")
            print("ğŸ“„ Fichier 'failed_urls.txt' crÃ©Ã© avec les pages Ã  relancer.")

    # âœ… Supprime les None avant de les envoyer Ã  Meilisearch
    final.extend(scrapped_data)  # ou final = [r for r in scrapped_data if r is not None]

    start_time = time.perf_counter()

    # âš™ï¸ 3ï¸âƒ£ Configuration des attributs
    index.update_filterable_attributes([
        "keyword",
        "denom_fallback_bce",
        "admins_detectes"  # ğŸ‘ˆ ici on ajoute le champ facetable
    ])
    index.update_searchable_attributes([
        "id", "date_doc", "title", "keyword", "date_jugement", "TVA",
        "extra_keyword",
        "administrateur", "text",
        "denoms_by_bce", "adresses_by_bce",
        "adresses_by_ejustice", "denoms_by_ejustice",
        "denom_fallback_bce"  # ğŸ†• ajoutÃ© ici
    ])

    index.update_displayed_attributes([
        "id", "date_doc", "title", "keyword", "extra_keyword",
        "date_jugement", "TVA",
        "administrateur", "text", "url",
        "denoms_by_bce", "adresses_by_bce",
        "adresses_by_ejustice", "denoms_by_ejustice",
        "denom_fallback_bce"  # ğŸ†• ajoutÃ© ici aussi
    ])

    print(f"[âš™ï¸] Index '{index_name}' prÃªt en {time.perf_counter() - start_time:.2f}s.")
    documents = []
    with requests.Session() as session:
        for record in tqdm(final, desc="PrÃ©paration Meilisearch"):
            cleaned_url = clean_url(record[4])
            date_jugement = None  # Valeur par dÃ©faut si record[14] est None
            if record[10] is not None:
                brut = clean_date_jugement(record[10])
                date_jugement = convertir_date(brut)  # <= on rÃ©cupÃ¨re le rÃ©sultat

            texte = record[3].strip()
            texte = texte.encode("utf-8", errors="replace").decode("utf-8", errors="replace")
            doc_hash = generate_doc_hash_from_html(record[3], record[1])  # âœ… Hash du texte brut + date
            # âœ… Construction du document avec administrateurs structurÃ©s
            admins = record[11] or []

            # --- Conversion en format unifiÃ© pour Meili + Postgres ---
            if isinstance(admins, list) and admins and isinstance(admins[0], dict):
                # cas nouveau format [{role, entity, raw}]
                admin_struct = [
                    {
                        "role": a.get("role", "inconnu"),
                        "entity": a.get("entity") or a.get("nom") or "",
                        "raw": a.get("raw", "")
                    }
                    for a in admins if isinstance(a, dict)
                ]
            elif isinstance(admins, list):
                # ancien format liste de chaÃ®nes
                admin_struct = [{"role": "inconnu", "entity": str(a), "raw": str(a)} for a in admins]
            elif isinstance(admins, str):
                # ancien format texte unique
                admin_struct = [{"role": "inconnu", "entity": admins, "raw": admins}]
            else:
                admin_struct = None

            admins_detectes = []
            if admin_struct:
                admins_detectes = [a["entity"] for a in admin_struct if a.get("entity")]
            doc = {
                "id": doc_hash,
                "date_doc": record[1],
                "lang": record[2],
                "text": record[3],
                "url": cleaned_url,
                "keyword": record[5],
                "TVA": record[6],
                "title": record[7],
                "subtitle": record[8],
                "extra_keyword": record[9],
                "date_jugement": record[10],
                "administrateur": admin_struct,  # ğŸ†• champ normalisÃ©
                "denoms_by_bce": record[13],
                "adresses_by_bce": record[14],
                "adresses_by_ejustice": record[15],
                "denoms_by_ejustice": record[16],
                "denom_fallback_bce": None,
                "admins_detectes": admins_detectes,  # âœ… nouveau champ pour MeiliSearch
            }

            documents.append(doc)
            # ğŸ” Indexation unique des dÃ©nominations TVA (aprÃ¨s avoir rempli documents[])
            print("ğŸ” Indexation des dÃ©nominations par TVA (1 seule lecture du CSV)â€¦")

    # --------------------------------------------------------------------------------------------------------------
    # LOGGERS EN CAS DE CHAMPS OBLIGATOIRE VIDE (Pour tous les mots clefs)
    # Champs obligatoires : id - date_doc - text - keyword -extra_keyword - TVA
    # --------------------------------------------------------------------------------------------------------------
    for doc in documents:
        missing = []
        if not doc.get("id"):
            missing.append("id")
        if not doc.get("date_doc"):
            missing.append("date_doc")
        if not doc.get("text"):
            missing.append("text")
        if not doc.get("keyword"):
            missing.append("keyword")
        if not doc.get("extra_keyword"):
            missing.append("extra_keyword")
        if not doc.get("TVA"):
            missing.append("TVA")

        if missing:
            logger_champs_manquants_obligatoires.warning(f"[âŒ Champs manquants] DOC={doc.get('id')} | Manquants: {missing}")

    # --------------------------------------------------------------------------------------------------------------
    # ğŸ§© Enrichissement BCE (uniquement depuis le fichier CSV local, sans fetch ni fallback)
    # --------------------------------------------------------------------------------------------------------------

    for doc in documents:
        # Initialisation systÃ©matique
        doc["denoms_by_bce"] = None
        doc["adresses_by_bce"] = None
        doc["denom_fallback_bce"] = None  # restera toujours None ici

        if "annulation_doublon" in (doc.get("extra_keyword") or []):
            continue

        tvas = doc.get("TVA") or []
        denoms_map = {}
        adresses_map = {}

        for tva in tvas:
            if not verifier_tva_belge(tva):
                logger_tva_invalide.warning(
                    f"[âŒ TVA invalide] {tva} | DOC={doc.get('id')} | URL={doc.get('url')}"
                )
                continue

            bce = format_bce(tva)
            if not bce:
                continue

            # ğŸ”¹ Lecture depuis les index locaux uniquement
            noms = sorted(denom_index.get(bce, []))
            type_ent = personnes_physiques.get(bce, "inconnu")

            if noms:
                denoms_map[bce] = {"type": type_ent, "noms": noms}

            adresses = []
            if bce in address_index:
                adresses.extend({"adresse": addr, "source": "siege"} for addr in address_index[bce])

            if bce in establishment_index:
                for etab in establishment_index[bce]:
                    etab_norm = re.sub(r"\D", "", etab)
                    if etab_norm in address_index:
                        adresses.extend(
                            {"adresse": addr, "source": "etablissement"}
                            for addr in address_index[etab_norm]
                        )

            if adresses:
                adresses_map[bce] = adresses

            # âŒ Aucun fallback ici : il sera gÃ©rÃ© dans enrichissement_ejustice_bce.py

        # âœ… Affectations finales
        doc["denoms_by_bce"] = (
            [{"bce": bce, "type": data["type"], "noms": data["noms"]}
             for bce, data in denoms_map.items()]
            if denoms_map else None
        )

        doc["adresses_by_bce"] = (
            [{"bce": bce, "adresses": adresses}
             for bce, adresses in adresses_map.items()]
            if adresses_map else None
        )

        # âš ï¸ On garde ce champ vide ici pour le prochain script
        doc["denom_fallback_bce"] = None

    # ğŸ§¼ Nettoyage des champs adresse : suppression des doublons dans la liste
    # ================================================================
    # âœ… VÃ©rification gÃ©nÃ©rique : cohÃ©rence nom/adresse pour chaque BCE
    # ================================================================
    for doc in documents:
        keyword = (doc.get("keyword") or "").lower()
        tvas = doc.get("TVA") or []
        denoms_by_bce = doc.get("denoms_by_bce") or []
        adresses_by_bce = doc.get("adresses_by_bce") or []

        # Indexer les noms et adresses par BCE au format "xxxx.xxx.xxx"
        denom_map = {
            format_bce(entry.get("bce")): entry.get("noms", [])
            for entry in denoms_by_bce if entry.get("bce")
        }
        addr_map = {
            format_bce(entry.get("bce")): entry.get("adresses", [])
            for entry in adresses_by_bce if entry.get("bce")
        }

        for tva in tvas:
            bce = format_bce(tva)
            if not bce:
                logger_tva_invalide.warning(
                    f"[âŒ TVA invalide] {tva} | DOC={doc.get('id')} | URL={doc.get('url')}"
                )
                continue

            noms = denom_map.get(bce, [])
            adrs = addr_map.get(bce, [])

            has_nom = bool(noms and any(isinstance(n, str) and n.strip() for n in noms))
            has_adresse = bool(
                adrs and any(
                    isinstance(addr, dict) and any(str(v).strip() for v in addr.values())
                    for addr in adrs
                )
            )

            if not has_nom and not has_adresse:
                logger_champs_manquants_csv_bce.warning(
                    f"[âš ï¸ BCE sans dÃ©nomination ni adresse] "
                    f"DOC={doc.get('id')} | BCE={bce} | keyword={keyword} | URL={doc.get('url')}"
                )

    # --------------------------------------------------------------------------------------------------------------
    #                                       VERIFIVATION FINALE AVANT D ENVOYER A MEILI
    # --------------------------------------------------------------------------------------------------------------
    if not documents:
        print("âŒ Aucun document Ã  indexer.")
        sys.exit(1)

    # ğŸ” Log des doublons avant dÃ©duplication
    print(f"[ğŸ“‹] Total initial de documents : {len(documents)}")
    hash_to_docs = defaultdict(list)
    for doc in documents:
        hash_to_docs[doc["id"]].append(doc)

    duplicates = {h: docs for h, docs in hash_to_docs.items() if len(docs) > 1}
    if duplicates:
        print(f"[âš ï¸] {len(duplicates)} doublons dÃ©tectÃ©s avant nettoyage :")
        for h, docs in duplicates.items():
            print(f" - id = {h} (Ã—{len(docs)})")
            for d in docs:
                print(f"    â€¢ URL: {d['url']} | Date: {d['date_doc']}")
    else:
        print("[âœ…] Aucun doublon dÃ©tectÃ© avant nettoyage.")

    # ğŸ” Garde uniquement le plus rÃ©cent par id
    unique_docs = {}
    for doc in sorted(documents, key=lambda d: d["date_doc"], reverse=True):
        unique_docs[doc["id"]] = doc
    documents = list(unique_docs.values())

    print(f"[âœ…] Total aprÃ¨s dÃ©duplication : {len(documents)}")

    # ğŸš€ Indexation vers Meilisearch
    batch_size = 3000
    task_ids = []
    print(f"[ğŸ“¦] Indexation de {len(documents)} documents en batchs de {batch_size}â€¦")

    for i in tqdm(range(0, len(documents), batch_size), desc="Envoi vers Meilisearch"):
        batch = documents[i:i + batch_size]
        task = index.add_documents(batch)
        task_ids.append(task.task_uid)

    # ğŸ•’ Attente de la complÃ©tion
    print(f"[â³] Attente de {len(task_ids)} tÃ¢ches Meiliâ€¦")
    for uid in task_ids:
        client.wait_for_task(uid, timeout_in_ms=180_000)
        task_info = client.get_task(uid)
        print(f" - Task {uid} â†’ {task_info.status}")
        if task_info.status == "failed":
            print(f"   âŒ Erreur : {task_info.error}")

    # ğŸ“Š VÃ©rifie rÃ©sultat final
    stats = index.get_stats()
    print(f"[ğŸ“Š] Documents rÃ©ellement indexÃ©s dans Meili: {stats.number_of_documents}")
    # ===============================================================================================================

    #                     -> CSV APPEL ANNEXES MONITEUR BELGE
    #                     -> POSTGRE
    #                     -> json file reconstruction meili
    # ===============================================================================================================

    # --------------------------------------------------------------------------------------------------------------
    #                   FICHIERS CSV CONTENANT LES DONNES NECESSAIRES POUR LES APPELS AUX ANNEXES
    #                                             DU MONITEUR BELGE
    # --------------------------------------------------------------------------------------------------------------
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["tva", "id", "url", "keyword", "denoms_by_bce", "adresses_by_bce", "denom_fallback_bce"])
        for doc in documents:
            for tva in doc.get("TVA", []):
                writer.writerow([tva, doc["id"], doc["url"], doc["keyword"], doc["denoms_by_bce"],
                                 doc["adresses_by_bce"], doc["denom_fallback_bce"]])
    # --------------------------------------------------------------------------------------------------------------
    #                                                    BASE DE DONNEE : POSTGRE
    #
    # --------------------------------------------------------------------------------------------------------------
    print("[ğŸ“¥] Connexion Ã  PostgreSQLâ€¦")
    create_table_moniteur()
    insert_documents_moniteur(documents)

    # --------------------------------------------------------------------------------------------------------------
    #                                 FICHIERS CSV CONTENANT LES DONNES INSEREES DANS MEILI
    # --------------------------------------------------------------------------------------------------------------
    # ğŸ“ Sauvegarde finale en JSON local (version enrichie)
    os.makedirs("exports", exist_ok=True)
    json_path = os.path.join("exports", f"documents_enrichis_{keyword}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(documents, f, indent=2, ensure_ascii=False)

    print(f"[ğŸ’¾] Fichier JSON enrichi sauvegardÃ©: {json_path}")


if __name__ == "__main__":
    main()
